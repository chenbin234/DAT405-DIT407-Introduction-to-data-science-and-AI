{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rHoSDyYpdh-s"
   },
   "source": [
    "## Assignment 7: Neural Networks using Keras and Tensorflow\n",
    "\n",
    "Please see the associated document for questions\n",
    "\n",
    "|  <font size=4>Name</font>  | <font size=4>Working Hours</font>   |\n",
    "|  :----  | :----  |\n",
    "| <font size=4>Dimitrios Koutsakis</font>   | <font size=4>8</font> |\n",
    "| <font size=4>Bingcheng Chen</font>  | <font size=4>8</font> |\n",
    "\n",
    "If you have problems with Keras and Tensorflow on your local installation please make sure they are updated. On Google Colab this notebook runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02ZYZ-WmdhwH"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras import utils as np_utils\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJRCoRmew8Zd",
    "outputId": "8a74f963-06c8-4ba7-fb03-889e43dfa15e"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters data-loading and formatting\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "(x_train, lbl_train), (x_test, lbl_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-I3g1RrZ0wpI"
   },
   "source": [
    "#### **1. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UswCCQLS0s1I"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = keras.utils.to_categorical(lbl_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(lbl_test, num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1.1** Explain the data pre-processing highlighted in the notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Network model, training, and changing hyper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7Aer42gk1W9"
   },
   "outputs": [],
   "source": [
    "## Define model ##\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),\n",
    "        metrics=['accuracy'],)\n",
    "\n",
    "fit_info = model.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           verbose=1,\n",
    "           validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2.1** How many layers does the network in the notebook have? How many neurons does each layer have? What activation functions and why are these appropriate for this ap- plication? What is the total number of parameters for the network? Why do the input and output layers have the dimensions they have?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2.2** What loss function is used to train the network? What is the functional form (a math- ematical expression) of the loss function? and how should we interpret it? Why is it appropriate for the problem at hand?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2.3** Train the network for 10 epochs and plot the training and validation accuracy for each epoch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2.4** Update the model to implement a three-layer neural network where the hidden layers have 500 and 300 hidden units respectively. Train for 40 epochs. What is the best vali- dation accuracy you can achieve? Geoff Hinton (a co-pioneer of Deep learning) claimed this network could reach a validation accuracy of 0.9847 (http://yann.lecun.com/exdb/mnist/) using weight decay (L2 regularization of weights(kernels): https://keras.io/api/layers/regularizers/). Implement weight decay on hidden units and train and select 5 regularization factors from 0.000001 to 0.001. Train 3 replicates networks for each regularization factor. Plot the final validation accuracy with standard deviation (computed from the replicates) as a function of the regularization factor. How close do you get to Hintons result? – If you do not get the same results, what factors may influence this? (hint: What information is not given by Hinton on the MNIST database that may influence Model training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Convolutional layers**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3.1** Design a model that makes use of at least one convolutional layer – how performant a model can you get? -- According to the MNIST database it should be possible reach to 99% accuracy on the validation data. If you choose to use any layers apart from the convolutional layers and layers that you used in previous questions, you must describe what they do. If you do not reach 99% accuracy, report your best performance, and explain your attempts and thought process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3.2** Discuss the differences and potential benefits of using convolutional layers over fully connected ones for the application?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_7_NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
